\documentclass[12pt, a4paper]{article}

\usepackage[utf8]{inputenc}  
\usepackage[T1]{fontenc}  
\usepackage[francais]{babel}

\usepackage{graphicx}
\usepackage{xspace}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{geometry}
\geometry{hmargin=3.5cm,vmargin=3.5cm}

%%%%%%%%%%%%%%%%%%%%%%%%%% Notations %%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\A}{A} % matrice complète
\newcommand{\B}{B} % matrice incomplète
\newcommand{\Ap}{\widehat{A}} % matrice approximée

\newcommand{\Ac}[2]{a_{#1,#2}} % matrice complète composante
\newcommand{\Bc}[2]{b_{#1,#2}} % matrice incomplète composante
\newcommand{\Apc}[2]{\widehat{a}_{#1,#2}} % matrice approximée composante

\newcommand{\rmse}[1]{\mathrm{RMSE}\pp{#1}} % root mean square error
\newcommand{\norme}[2]{\left\lVert #1 \right\rVert_{#2}} % norme
\newcommand{\tnorme}[2]{\left|\hspace{-1pt}\left\lVert #1
			\right\rVert\hspace{-1pt}\right|_{#2}} % norme

\newcommand{\tr}[1]{#1^{\mathrm{T}}} % Trace
\newcommand{\segint}[2]{\left[\!\left[#1,\: #2\right]\!\right]} % Trace
\newcommand{\ei}[1]{\mathrm{e}_i} % Vecteur de base

%raccourcis
\newcommand{\pp}[1]{\left(#1\right)} % parenthèses
\newcommand{\sdl}{

~

} % saut de ligne
%%%%%%%%%%%%%%%%%%%%%%%%%% Notations %%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Recommandation - Rapport partiel}
\author{Ken Chanseau--Saint-Germain \& Vincent Vidal}
\date{\today}
\maketitle

\tableofcontents
\sdl\sdl
\section*{Notation}
On notera pour $p>0$ un réel, $X$ un vecteur et $M$ une matrice quelconque : \[
	\norme{X}{p} = \pp{\sum_i \left|x_i\right|^p}^{\frac{1}{p}} \hspace{2cm} 
	\norme{M}{p} = \pp{\sum_{i, j} \left|m_{i,j}\right|^p}^{\frac{1}{p}}
\]\[
	\tnorme{M}{p} = \sup_{\norme{x}{p}=1} \norme{Mx}{p}
\]
Et on posera $\norme{X}{0}$ le nombre de composantes non nulles de $X$. 

\newpage
\hspace{1cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%       %%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
On se donne ici $n$ personnes donnant des notes à $m$ objets.\newline
On notera $\Ac{i}{j}$ la note de l'individu $i$ sur l'objet $j$ ainsi que
$A = \pp{\Ac{i}{j}}_{\substack{1\leq i\leq n \\ 1 \leq j \leq m}}$
la matrice des notes.

On suppose ici que l'on n'a accès qu'à une matrice incomplète $\B$ obtenue
en annulant certaines composantes de $A$. Le but est alors de trouver
une bonne approximation $\Ap$ de $A$ à partir de $\B$.
\sdl

On prendra comme mesure d'approximation, l'erreur moyenne suivante~: \[
	\rmse{\Ap} = \sqrt{\frac{\sum_{i,j \in S} \pp{\Ac{i}{j} - \Apc{i}{j}}^2}{\mathrm{Card}\pp{S}}}
\]
où $S$ dont les éléments correspondent aux composantes oubliées dans la matrice $\A$.

En pratique, les données que l'on utilisera pour tester les algorithmes
correspondront à des matrices $\A$ incomplètes, et on utilisera
une matrice $\B$ plus incomplète que $\A$. La matrice d'approximation
$\Ap$ obtenue donne alors des coefficients dont on ne peut vérifier
la précision. L'erreur moyenne ne sera alors calculée que sur les
coefficients non nuls de $\A$.

%%%%%%%%%%%%%%%%%%%%%%%%%%       %%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Approximation basique}
%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Par moyenne}

On suppose ici que la note d'un objet est de la forme
$\Apc{i}{j} = m_{\Ap} + p_i + o_j$, avec $m_{\Ap}$ la moyenne des valeurs
de $\Ap$, $p_i$ la moyenne des notes recentrées qu'a donné la personne
$i$ et $o_j$ la moyenne des notes recentrées qu'a obtenu l'objet $j$.
C'est à dire : \[
	m_{\Ap} = \frac{\sum_{i,j}\Bc{i}{j}}{\norme{\B}{0}}  \hspace{1cm}
	p_i = \frac{\sum_{j}\Bc{i}{j} - m_{\Ap}}{\norme{\tr{\B}\ei{i}}{0}} \hspace{1cm}
	o_j = \frac{\sum_{i}\Bc{i}{j} - m_{\Ap}}{\norme{\B\ei{j}}{0}}
\]

% vvvvvvvvvvvvvvvvvvvv %
\begin{algorithm}[H]
\KwData{La matrice incomplète $\B$.}
\KwResult{La matrice d'approximation $\Ap$.}
$m \longleftarrow \mathrm{mean}\pp{\B}$\;
\For{$i\in\segint{1}{n}$}{
	$p\left[i\right]\longleftarrow 0$\;
	$card\longleftarrow 0$\;
\For{$j\in\segint{1}{m}$}{
	$p\left[i\right]\longleftarrow p\left[i\right] + \B\left[i,\: j\right]$\;
	\If{$b\left[i,\: j\right]\neq 0$}{$card\longleftarrow card + 1$\;}
}
$p\left[i\right]\longleftarrow p\left[i\right]/card - m$\;
}
On calcul de même les $o\left[j\right]$\;
\For{$i\in\segint{1}{n}$}{
\For{$j\in\segint{1}{m}$}{
	$\Ap\left[i,\: j\right] \longleftarrow m + p\left[i\right] + o\left[j\right]$\;
}}
\Return{$\Ap$}\;
 \caption{Recommandations naïves par moyenne}
\end{algorithm}
% ^^^^^^^^^^^^^^^^^^^^ %

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Par minimisation de problème convexe}

De manière plus précise, on peut résoudre le problème suivant de
minimisation convexe pénalisée sur les $p_i$ et $o_j$ pour trouver
des $p_i$ et $o_j$ plus adaptés~: \[
	\mathcal{P}\pp{p,\:o} = \sum_{i,j} \pp{\Ac{i}{j} - m_{\Ap} + p_i + o_j}^2 + \lambda\pp{\norme{o}{1} + \norme{p}{1}}
\]
Problème que l'on peut écrire : \[
	\mathcal{P}\pp{x} = \norme{Mx + b}{2}^2 + \lambda\norme{x}{1}.
\]
Il suffit alors de résoudre ce problème, pour un $\lambda$ donné
et de calculer l'approximation à partir des coefficients $p_i$ et $o_j$ obtenus. 
\sdl
On utilisera ici un algorithme de minimisation qui descend de l'algorithme
de Douglas-Rachford \cite{douglas},
se rapprochant d'une méthode de descente de gradient. Cet algorithme
prend un paramètre supplémentaire, $\mu$, dont dépend la vitesse de
convergence. La suite calculée par cet algorithme converge vers un minimum
lorsque $\mu$ est suffisamment proche de $0$.
\sdl
% vvvvvvvvvvvvvvvvvvvv %
\begin{algorithm}[H]
\KwData{$x$, $\lambda$, le nombre d'itération $N$ et un paramètre $\mu$.}
\KwResult{une meilleur approximation $x$.}

\For{$k\in\segint{1}{N}$}{
$x\longleftarrow x - \mu.\tr{M}\pp{Mx+b}$\;
	\tcc{Seuillage doux sur chaque composante}
\For{$i\in\segint{1}{n}$}{
$x\left[i\right]\longleftarrow \mathrm{signe}\pp{x\left[i\right]}.\max\pp{\:\left|x\left[i\right]\right| - \mu\lambda,\:0\:}$\;
}
}
\Return{$x$}\;
 \caption{Minimisation convexe}
\end{algorithm}
% ^^^^^^^^^^^^^^^^^^^^ %

%%%%%%%%%%%%%%%%%%%%%%%%%%       %%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Approximation par voisinage}

Nous noterons par la suite $P_i$ la $i$-ième colonne de $\tr{B}$,
représentant la personne $i$ et $O_j$ la $j$-ième colonne de $\B$,
représentant l'objet $j$.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Plus proches voisins}

La méthode précédente suppose qu'il n'existe qu'un seul type de personne
et d'objet : un objet est bien ou mauvais dans l'ensemble et une
personne donne de manière générale des notes bonnes ou des notes
mauvaises. Ceci apparaît comme très naïf.
\sdl

On considère ici une méthode différente. Pour une personne $i$ et
un objet $j$ dont l'information est manquante, nous allons chercher
parmi les personnes ayant noté $j$ les $k$ qui se rapprochent le plus
$i$. En notant $S$ les indices de ces personnes,
on peut alors prendre comme approximation la
moyenne pondérée des notes~: \[
	\Apc{i_0}{j} = \frac{\underset{^{i \in S}}{\sum}\pp{ s\pp{i_0,\:i}\!\cdot\!\Bc{i}{j}}}%
				{\underset{^{i \in S}}{\sum}\left|s\pp{i_0,\:i}\right|}. \]

Où $s(i,j)$ correspond à la distance entre deux personnes et représente leur ressemblance. Il reste à définir une distance pour sélectionner le plus proche voisin.
Si on cherche le plus proche voisin d'un vecteur $X$ de support
$S$ parmi un ensemble de vecteurs $E$, on pourrait considérer $E_{|S}$
les vecteurs de $E$ projetés sur le support $S$ et on prendrait alors
le plus proche voisin pour la norme euclidienne sur
$\mathbb{R}^{\mathrm{Card}\pp{S}}$.

Nous prendrons ici
la valeur du cosinus de l'angle entre les deux personnes~: \[
	s\pp{i_1,\:i_2} = \frac{\tr{P_{i_1}}\!\cdot\!P_{i_2}}{\norme{P_{i_1}}{2}\norme{P_{i_2}}{2}}.
\]

\begin{algorithm}[H]
\KwData{La matrice $\B$ et le nombre $k$ de voisins pris en compte.}
\KwResult{La matrice approchée $\Ap$.}

calcul des $s(i_1,i_2)$\;
\For{$i\in\segint{1}{n}$}{
\For{$j\in\segint{1}{m}$}{
 $S \longleftarrow$ indices des $k$-plus proches voisins de $i$ ayant noté $j$ \;
 	$\Apc{i}{j} \longleftarrow {\underset{^{t \in S}}{\sum}\pp{ s\pp{i,\:t}\!\cdot\!\Bc{t}{j}}}/
				{\underset{^{t \in S}}{\sum}\left|s\pp{i,\:t}\right|}$\;
}
}

\Return{$\Ap$}\;

\caption{Recommandation par $k$-plus proches voisins}

\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implantation matricielle pour $k = n$}

Dans le cas où $k = n$, les calculs peuvent s'effectuer plus efficacement à l'aide de calculs matriciels.
On pourra considérer l'algorithme suivant~:

\sdl
% vvvvvvvvvvvvvvvvvvvv %
\begin{algorithm}[H]
\KwData{La matrice $\B$.}
\KwResult{La matrice approchée $\Ap$.}
$S \longleftarrow B\tr{B}$\;
\For{$i\in\segint{1}{n}$}{
$norm\left[i\right] \longleftarrow \norme{\tr{\B}\ei{i}}{2}$
}
\For{$i\in\segint{1}{n}$}{
$somme\left[i\right] \longleftarrow 0$\;
\For{$k\in\segint{1}{n}$}{
	$S\left[i,\: k\right] \longleftarrow S\left[i,\: k\right]/\pp{norm\left[i\right].norm\left[k\right]}$\;
$somme\left[i\right] \longleftarrow somme\left[i\right] + \left|S\left[i,\: k\right]\right|$\;
}}
$\Ap \longleftarrow S.\B$\;
\For{$i\in\segint{1}{n}$}{
\For{$j\in\segint{1}{m}$}{
	$\Ap\left[i,\: j\right] \longleftarrow \Ap\left[i,\: j\right]/
		somme\left[i\right]$\;
}}
\Return{$\Ap$}\;
 \caption{Recommandation par voisinage}

\end{algorithm}
% ^^^^^^^^^^^^^^^^^^^^ %

%%%%%%%%%%%%%%%%%%%%%%%%%%       %%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Décomposition en valeurs singulières}

Le but est ici de faire apparaître $k$ modèles de personnes,
dont toutes les autres seront des combinaisons linéaires.

Autrement dit, on souhaite faire apparaître la matrice $\Ap$
de la manière suivante : \[
	\Ap = P\cdot O
\]
avec $P \in \mathcal{M}_{n,\:k}\pp{\mathbb{R}}$ de colonnes libres que l'on
peut prendre orthonormées et $O \in \mathcal{M}_{k,\:m}\pp{\mathbb{R}}$.

On peut facilement accéder à une telle décomposition à l'aide de
la décomposition en valeurs singulières.

\sdl
% vvvvvvvvvvvvvvvvvvvv %
\begin{algorithm}[H]
\KwData{La matrice $\B$ et un entier $k$.}
\KwResult{La matrice approchée $\Ap$.}

$\left[ U,\: S,\: V\right] \longleftarrow \mathrm{SVD}\pp{\B}$\;
\For{$i\in\segint{k}{\mathrm{size}\pp{S}}$}{
$S\left[i,\:i\right] \longleftarrow 0$\;
}
$\Ap \longleftarrow U S \tr{V}$\;
\Return{$\Ap$}\;
 \caption{Recommandation par décomposition en valeurs singulières}
\end{algorithm}
% ^^^^^^^^^^^^^^^^^^^^ %

%%%%%%%%%%%%%%%%%%%%%%%%%%       %%%%%%%%%%%%%%%%%%%%%%%%%%
\nocite{*}
\bibliographystyle{alpha}
\bibliography{bibliographie}

\end{document}